{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Homework1-my-glove-implementation-submit-version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c9cc470553dc40bcb1226c818a9f1a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4463381fa07f4c81bd449111b268ba94",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_783326d2d01c41918b4f4225e8710f74",
              "IPY_MODEL_eeba70707ada46e5842f63a27261d5ef"
            ]
          }
        },
        "4463381fa07f4c81bd449111b268ba94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "783326d2d01c41918b4f4225e8710f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6a5e6d7839a74fc1a65adc0529432f2e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 57340,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 57340,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ae465062c944ee28db7142f6ed2ae8a"
          }
        },
        "eeba70707ada46e5842f63a27261d5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00851413a61a42559810d1bd98519b5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 57340/57340 [00:31&lt;00:00, 1828.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e029116b7aa546309953be691751198d"
          }
        },
        "6a5e6d7839a74fc1a65adc0529432f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ae465062c944ee28db7142f6ed2ae8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00851413a61a42559810d1bd98519b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e029116b7aa546309953be691751198d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN-sVUVLOU0x"
      },
      "source": [
        "## Assigment 1. Part b. Implemention GloVE\n",
        "\n",
        "*Done by Polina Turishhcheva, Fall 2020*\n",
        "\n",
        "### Summary from GloVE Algorythm :\n",
        " * Based on the text corpus build a matrix X, which would show how often a word x appear with word y such that there no more than n-2 words between them, where n is size of the context window.\n",
        " * Than we randomly initiate a matrix W of size (2*vocab_size, vector_size), which is doubled because each word has a vector for it in a role of main word and in the role of a context word. We also randomly initiate matrixes for biases and gradients. Than we start the training, which is iterating over the pairs of words and changing the values in all randmoly initiaited matrixes according to AdaGrad methodology.\n",
        " The cost function from the article is   \n",
        " $\\sum_{i,j=1}^{vocab}f_{i,j}*(w_i^T w_j~ + b_i +b_j - log(f_{i,j}))^2$  \n",
        " where $f_{i,j}$ is score from coocurennce matrix\n",
        " AdaGrad updates are   \n",
        " score from coocurennce matrix and $b_i , b_j$  are constant, with respenct to $w_i, w_j$, hence, when we derivate we are going to derivate a complex function, the $f_{i,j}$ constant will stay, the corresponding multiplier will go out and the power will decrease. We will get something like\n",
        " $J'_{w_i} = \\sum_{j=1}^{vocab} f_{i,j}*w_j*(w_i^T w_j~ + b_i +b_j - log(f_{i,j}))$\n",
        " and AdaGrad makes updates as $x (t+1, i) = x(t,i)-\\frac{mu*g_(t,i)}{\\sum_{t'=1}^{t-1} g^2_{t',i}}$\n",
        "\n",
        "\n",
        "\n",
        "**Choice of hyperparameters** \n",
        "* Based on https://www.aclweb.org/anthology/D14-1162.pdf :\n",
        "  * context window size = 10\n",
        "  * α = 3/4 (\"We [empirically] found that α = 3/4 gives a mod- est improvement over a linear version with α = 1\")\n",
        "  * 50 iterations (\"We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise\")\n",
        "  * learning_rate = 0.05 and AdaGrad optimizer (\"train the model using AdaGrad (Duchi et al., 2011), stochastically sampling non- zero elements from X, with initial learning rate of 0.05.\")\n",
        "\n",
        "  I have used the output vector size of length 100 for because colab provides limited amount of memory\n",
        "\n",
        "**Disclaimer: this code takes a lot to work**\n",
        "* I have tried to put training to GPU but it appeared to be even longer (around 90 min per iteration instead of 9 minutes). Eeverything was put to GPU memory and most of the operations were interchanged with PyTorch build-in operations but it did not helped still. Maybe using dataloader can help but anyway, I do not expect it to work 10 times faster after including DataLoader, hence, using CPU appeared to be better in this case.\n",
        "* Colab has just one core, hence, speeding up via multiprocessing would not work\n",
        "* Sometimes it crashes on the stage of training for Gutenberg dataset due to lack of RAM but reruning from the begining usually helps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQFXI1jJHaYF",
        "outputId": "f768ed28-2ef8-4d96-aff2-63475804d184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!gdown --id 1eMndxf7ng32-CZeYrCxlYevcjyZIOgac "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eMndxf7ng32-CZeYrCxlYevcjyZIOgac\n",
            "To: /content/BATS_3.0.zip\n",
            "\r  0% 0.00/64.3k [00:00<?, ?B/s]\r100% 64.3k/64.3k [00:00<00:00, 56.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjtpMKEV71gw",
        "outputId": "5c9c98b6-dc8c-4b2f-9cd4-eb8a58664bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!gdown --id 1ainkSl2ThLjNq9W4W9orH3CGdZwew-zK"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ainkSl2ThLjNq9W4W9orH3CGdZwew-zK\n",
            "To: /content/brown_raw_W.npy\n",
            "75.1MB [00:03, 22.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n2nCGSD71rl",
        "outputId": "669034eb-aee5-4729-a804-36217459781f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!gdown --id  1VbC3crotxedMd5u3ZtkHoUl3k-xQ51Ht"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VbC3crotxedMd5u3ZtkHoUl3k-xQ51Ht\n",
            "To: /content/gutenberg_raw_embeddings_dataset.npy\n",
            "73.7MB [00:00, 143MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7nX_2a_Uc7J"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ-64xPnIPfM"
      },
      "source": [
        "!pip install vecto\n",
        "clear_output()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiI4LmCYjNIY"
      },
      "source": [
        "! unzip BATS_3.0.zip\n",
        "clear_output()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKEW6lQP5sJn"
      },
      "source": [
        "import vecto"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr4-wkATVs3n"
      },
      "source": [
        "## Choosing Data\n",
        "Data for corpus :Project Gutenberg Selections and Brown Dataset from http://www.nltk.org/nltk_data/\n",
        "\n",
        "Preprocessing inspired by :\n",
        "* https://shravan-kuchkula.github.io/scrape_clean_normalize_gutenberg_text/#part-1-scrape-the-gutenberg-website : We have to lowercase everything and remove redundant characters (non-letter onces except the hypen symbol as it may be a part of word : co-operation).\n",
        "Because the datasets are formed from published books or articles, I assume that there is no need in spell-checking. Also I do not change the word forms to infinitives because I want to tests their derivational and inflectional morphologies further (based on BATS datasets). Words not characters are assumed to be tokens. In fact, this is not the best choice from morfological perspective (because it results in 'cat' and 'cats' to be to non-connected words), characters as minimal unit may help to solve it but per-character processing is longer and more difficult to implement, hence, I have decided to stop at the word-level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIaEmNq0VcAy",
        "outputId": "feaee7e3-fecf-441e-85ab-95cdc4bf2b41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42CLvY_YaRa1",
        "outputId": "f2d81550-80cf-4354-9d03-da983c4a12c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "text_names = nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxlhZT8yvWdc"
      },
      "source": [
        "def preprocess_text(word):\n",
        "  # transforms everything to lower case, \n",
        "  # removes all puctuation and numbers except of dash symbol (non-invasive is one word not 2)\n",
        "  # removes redundant spaces\n",
        "  word = word.lower()\n",
        "  part1 = re.sub(r'[^\\w\\s-]',\"\", word)\n",
        "  part2 = re.sub(r'[\\d+]',\"\", part1)\n",
        "  part3 = re.sub(r' +', \" \", part2)\n",
        "  part4 = re.sub(r'_', \"\", part3)\n",
        "  part5 = re.sub(r' - ', \"-\", part4)\n",
        "  return  re.sub(r' -- ', \" \", part5).strip()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU8OKSAHS9eQ"
      },
      "source": [
        "def one_text_process(unique_words, sentences):\n",
        "  sentences = [preprocess_text(\" \".join(list_of_words)) for \n",
        "                    list_of_words in sentences]\n",
        "\n",
        "  unique_words_from_text =  list(set(\" \".join(sentences).split(' ')))\n",
        "  unique_words_from_text = filter(\n",
        "      lambda x: set(x) != set('-') and set(x) != set(''), unique_words_from_text)\n",
        "  \n",
        "  for w in unique_words_from_text :\n",
        "    unique_words.add(w)\n",
        "\n",
        "  return unique_words\n",
        "\n",
        "def only_unique_words(mode='brown'): \n",
        "  unique_words = set()\n",
        "  vocab_size = 0\n",
        "  enumerated_words = {}\n",
        "  if mode == 'gutenberg':\n",
        "    text_names = nltk.corpus.gutenberg.fileids()\n",
        "    for i in text_names:\n",
        "      sentences = nltk.corpus.gutenberg.sents(i)\n",
        "      unique_words = one_text_process(unique_words, sentences)\n",
        " \n",
        "  else:\n",
        "    sentences = brown.sents()\n",
        "    unique_words = one_text_process(unique_words, sentences)\n",
        "\n",
        "  unique_words = sorted(unique_words)\n",
        "\n",
        "  for i, w in enumerate(unique_words):\n",
        "    vocab_size += 1\n",
        "    enumerated_words[w] = i\n",
        "  return vocab_size, enumerated_words"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ipy2XACeB-"
      },
      "source": [
        "# we have ~40 000 unique words in our corpus\n",
        "# and ~ 2 154 000 words in total\n",
        "# put mode='gutenberg' to change the dataset\n",
        "vocab_size, enumerated_words = list(only_unique_words())\n",
        "# vocab_size, enumerated_words = list(only_unique_words('gutenberg'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5-b2ibScIqG"
      },
      "source": [
        "!mkdir vocab"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYZ_vsLXcgLt"
      },
      "source": [
        "with open('vocab/dataset.vocab', 'w') as f:\n",
        "  f.write('\\n'.join(sorted(enumerated_words.keys())))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCBQQemJKQHJ"
      },
      "source": [
        "W = np.load('brown_raw_W.npy')\n",
        "# comment the previous and uncomment currrent line to change the dataset\n",
        "# W = np.load('gutenberg_raw_embeddings_dataset.npy')\n",
        "\n",
        "# normalize embeddings\n",
        "for i, row in enumerate(W):\n",
        "    W[i, :] /= np.linalg.norm(row)\n",
        "    \n",
        "# Remove context word vectors\n",
        "W = W[:len(enumerated_words), :]\n",
        "\n",
        "np.save('vocab/embeddings_dataset_normalized.npy', W)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpYmmSFXE7oh"
      },
      "source": [
        "## Creating a Co-Occurence matrix\n",
        "Now, when we have a set of unique words, we can preprocess initial texts and create a co-ocurence matrix. We will use the window size of 10 and find how often each pair of words appear in one sentence inside the window size. \n",
        "\n",
        "Actually, colab RAM is limited, hence, it was not possible to make dataFrame with size (number_of_unique_words, number_of_unique_words) or a dictionary with all possible combinations of the unique words. scipy.sparse. lil_matrix has worked in terms of memory but it takes really long to work, due to linear time to access. Hence, I have desided to add turple of words to a dictionary as a key if and only if the were found together. There may be the case that a same word would appear twice inside the context window, such cases are also saved. The order of words in pair is neglected (It does not matter either is was 'a' before 'b' or vice versa inside the context window. It will be added for both ('a', 'b') key and ('b', 'q') key. This is done just to make a context window in the same fashion as a moving one, just going forward, but for a particular word token both before and after it should be counted as ones inside the context window. If I have used a co-ocurance matrix in its normal form - it would be symmetric.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7YZgEduSnsX"
      },
      "source": [
        "def one_text_matrix(coocur_dict, sentences, enumerated_words, window_size = 10):\n",
        "  sentences = [preprocess_text(\" \".join(list_of_words)) for list_of_words in sentences]\n",
        "  for sents in tqdm(sentences):\n",
        "    sents_split = sents.split()\n",
        "    sents_split = list(filter(\n",
        "      lambda x: set(x) != set('-') and set(x) != set(''), sents_split))\n",
        "    l = len(sents_split)\n",
        "    for i, word1 in enumerate(sents_split):\n",
        "      subsents = sents_split[i+1 : max(i + window_size, l)]\n",
        "      word1 = enumerated_words[word1]\n",
        "      for j, word2 in enumerate(subsents):\n",
        "        word2 = enumerated_words[word2]\n",
        "        name1 = (word2, word1)\n",
        "        name2 = (word1, word2)\n",
        "        if name1 in coocur_dict.keys():\n",
        "          coocur_dict[name1] += 1 / (j + 1)\n",
        "          coocur_dict[name2] += 1 / (j + 1)\n",
        "        else:\n",
        "          coocur_dict[name1] = 1 / (j + 1)\n",
        "          coocur_dict[name2] = 1 / (j + 1)\n",
        "\n",
        "  return coocur_dict\n",
        "\n",
        "def build_cooucurence(enumerated_words, mode='brown', window_size = 10):\n",
        "  coocur_dict = {}\n",
        "  if mode == 'brown':\n",
        "    sentences = brown.sents()\n",
        "    coocur_dict = one_text_matrix(coocur_dict, sentences, \n",
        "                                  enumerated_words, window_size = 10)\n",
        "    \n",
        "  else:  \n",
        "    text_names = nltk.corpus.gutenberg.fileids()\n",
        "    for text in tqdm(text_names):\n",
        "      sentences = nltk.corpus.gutenberg.sents(text)\n",
        "      coocur_dict = one_text_matrix(coocur_dict, sentences, \n",
        "                                  enumerated_words, window_size = 10)\n",
        "          \n",
        "  average_coocur_score = sum(coocur_dict.values())/len(coocur_dict.values())\n",
        "  print(f'Average score : {average_coocur_score}')\n",
        "  # for i in coocur_dict.keys():\n",
        "  # saving indexes, not words, allows to save RAM\n",
        "  coocur_dict = [[i[0], i[1], coocur_dict[i]] for i in sorted(coocur_dict.keys())]\n",
        "\n",
        "  return coocur_dict"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kPbe0J2VwZo",
        "outputId": "ba6112ab-0b1e-47d1-9acb-38d126827718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "c9cc470553dc40bcb1226c818a9f1a46",
            "4463381fa07f4c81bd449111b268ba94",
            "783326d2d01c41918b4f4225e8710f74",
            "eeba70707ada46e5842f63a27261d5ef",
            "6a5e6d7839a74fc1a65adc0529432f2e",
            "3ae465062c944ee28db7142f6ed2ae8a",
            "00851413a61a42559810d1bd98519b5c",
            "e029116b7aa546309953be691751198d"
          ]
        }
      },
      "source": [
        "# there are ~12 974 000 pairs of words - gutenberg\n",
        "#           ~ 7 174 000                - brown\n",
        "# please wait, this cell would take around 90 seconds\n",
        "# put mode='gutenberg' to change the dataset\n",
        "coocurs = build_cooucurence(enumerated_words)\n",
        "# coocurs = build_cooucurence(enumerated_words, mode ='gutenberg')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9cc470553dc40bcb1226c818a9f1a46",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=57340.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average score : 0.7536340860230532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6XOtyherW9_"
      },
      "source": [
        "# TO SAVE MEMORY\n",
        "enumerated_words = None"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENAfRhmEe2c"
      },
      "source": [
        "def train_glove(cooccurrences, vocab_size, vector_size = 100, \n",
        "                iterations = 50, learning_rate = 0.05, x_max = 100, alpha = 0.75):\n",
        "  '''\n",
        "  cooccurrences is dict (word1, word2)->score\n",
        "  vector_size is the size of an output word-embedding vector\n",
        "  Acknoledgement : \n",
        "  those lines are inspired by http://www.foldl.me/2014/glove-python/#implementation \n",
        "  '''\n",
        "\n",
        "  border = 0.5 / float(vector_size + 1)\n",
        "  # vector i is for word as a main one, \n",
        "  # vector i+vocab_size for the same word but in role of context word\n",
        "  W = np.random.uniform( (-1)*border, border, size = (vocab_size * 2, vector_size)) \n",
        "  biases = np.random.uniform( (-1)*border, border, size = (vocab_size * 2, 1))\n",
        "  gradient_squared = np.ones((vocab_size * 2, vector_size), dtype=np.float64)\n",
        "  gradient_squared_biases = np.ones(vocab_size * 2, dtype=np.float64)\n",
        "  data = []\n",
        "\n",
        "  for  i_main, i_context, cooccurrence in tqdm(cooccurrences):\n",
        "    # using pure indexes makes it a minute longer per iteration to run \n",
        "    # (which as ~ an additional hour for 50 iterations)\n",
        "    data.append((W[i_main], W[i_context + vocab_size],\n",
        "            biases[i_main],\n",
        "            biases[i_context + vocab_size],\n",
        "            gradient_squared[i_main], \n",
        "            gradient_squared[i_context + vocab_size],\n",
        "            gradient_squared_biases[i_main],\n",
        "            gradient_squared_biases[i_context + vocab_size],\n",
        "            cooccurrence))\n",
        "\n",
        "  # clering to save some space\n",
        "\n",
        "  costs = []\n",
        "  for i in tqdm(range(iterations)):\n",
        "    global_cost = 0\n",
        "    cost = 0\n",
        "    # this is inplace operation\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    for i in tqdm(range(len(data))):\n",
        "\n",
        "      v_main, v_context, b_main, b_context, gradsq_W_main, gradsq_W_context, gradsq_b, gradsq_b_c, cooccurrence = data[i]\n",
        "\n",
        "      # formula (9) from the article\n",
        "      weight = 0\n",
        "      if cooccurrence < x_max:\n",
        "        # ** is better than np.power\n",
        "        # because for np.power overflow occur earlier\n",
        "        weight = (cooccurrence / x_max) ** alpha\n",
        "      else:\n",
        "        weight = 1\n",
        "\n",
        "      # inside the brackets - left part of formula 8\n",
        "      cost_inner = (v_main @ v_context \n",
        "                      + b_main + b_context - np.log(cooccurrence))\n",
        "      \n",
        "      # formula 8 finished - one item inside summation\n",
        "      cost = weight * (cost_inner ** 2)\n",
        "\n",
        "      # variable to get the result for cost(formula 8)\n",
        "      global_cost += cost\n",
        "\n",
        "      # AdaGrad starts from here\n",
        "      # during taking gradients we assume Note that since f(Xij) is a constant \n",
        "      # as it depended only on dataset\n",
        "      # other details about diffirentiating in the text above\n",
        "\n",
        "      # AdaGrad is \n",
        "      # x_(t+1, i) = x_(t,i)-mu*g_(t,i)/()\n",
        "\n",
        "      scale = weight * cost_inner\n",
        "      lr_scale = learning_rate * scale\n",
        "\n",
        "      v_main -= lr_scale * v_context / (gradsq_W_main ** (1 / 2))\n",
        "      v_context -= lr_scale * v_main / (gradsq_W_context ** (1 / 2))\n",
        "\n",
        "      b_main -= lr_scale / (gradsq_b ** (1 / 2))\n",
        "      b_context -= lr_scale / (gradsq_b_c ** (1 / 2))\n",
        "\n",
        "      # sum grads for AdaGrads\n",
        "      gradsq_W_main += (scale * v_context) ** 2\n",
        "      gradsq_W_context += (scale * v_main) ** 2\n",
        "      gradsq_b += scale ** 2\n",
        "      gradsq_b_c += scale ** 2\n",
        "\n",
        "    print(global_cost)  \n",
        "    costs.append(global_cost) \n",
        "\n",
        "  return W, costs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jAIp6BdWnCM"
      },
      "source": [
        "# here it is just one itteration to show that the pipeline works\n",
        "# but you can change the number of iterations if you want to wait and test everything)\n",
        "vector_size = 100\n",
        "W, costs = train_glove(coocurs, vocab_size, \n",
        "                       vector_size = vector_size,iterations=1)\n",
        "clear_output()\n",
        "\n",
        "np.save('raw_W.npy', W)\n",
        "\n",
        "# W_avg = np.zeros((vocab_size, vector_size), dtype=np.float64)\n",
        "# # taking avearge between context and main word embeddings\n",
        "# for i in range(vocab_size):\n",
        "#   W_avg[i] = (W[i] + W[i + vocab_size]) / 2\n",
        "\n",
        "# np.save('dataset_avg.npy', W_avg)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbtTqIGXbOP"
      },
      "source": [
        "print(costs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocpjAHDCsDCF"
      },
      "source": [
        "## BATS EVALUATON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3D5-dLvyTPq"
      },
      "source": [
        "Evaluation adopted from the official tutorial\n",
        "* Please, note that documentation is not correct and vecto.benchmarks.analogy.analogy.run(model, options) does not work (https://github.com/vecto-ai/vecto/issues/73) \n",
        "\n",
        "* Also nornalize flag is not available from command line interface and it seems that in analogy method no normalization is required according to documentation, however, output assumes that the input embeddings are normalized. I have tried to analyze output for both normalized and non-normalized input embeddings\n",
        "\n",
        "* The evaluation is also very long to run (around 5 hours). I understand that it is not fun at all, hence, here is the link google colab with output files : https://drive.google.com/drive/folders/1xCh1S6IeQ24RsDITs9AgKSVqUIx8w35m?usp=sharing \n",
        "\n",
        "* Gutenberg scores (correct out of 24500)\n",
        "  * Inflectional_morphology - 21 (performed best on one-many nouns and simple past forms of vorbs (-ed))\n",
        "  * Derivational_morphology - 0\n",
        "  * Encyclopedic_semantics - 21 (performed best on questions about colors, I guess that is because fiction implies a lot of description also about colors)\n",
        "  * Lexicographic_semantics - 26 (performed best on questions about animal group names (flock/herd) and materials (steel/wood))\n",
        "\n",
        "* Brown non-normalized scores \n",
        "  * Inflectional_morphology - 15\n",
        "  * Derivational_morphology - 5\n",
        "  * Encyclopedic_semantics - 6\n",
        "  * Lexicographic_semantics - 57\n",
        "\n",
        "* Brown normalized scores \n",
        "  * Inflectional_morphology - 23 (performed best on vorbs present and past forms (-s, -ed, spend-spent))\n",
        "  * Derivational_morphology - 2\n",
        "  * Encyclopedic_semantics - 11 (questions about animals anologies and colors but color performance is worse compared to Gutenberg)\n",
        "  * Lexicographic_semantics - 60\n",
        "\n",
        "\n",
        "## Questions from the Assigment \n",
        "\n",
        "* **Think about how well the embedding implementation performed, e.g. did you have enough data?** \n",
        "Honestly, my embeddings seemed to perfom very poor on both datasets - brown and gutenberg ones. It is not surprising that using big context window resulted in poor catching of grammar features (derivational and inflectional morphologies). The results of sematic tasks is better but stil is far from being good. Experimenting with 2 datasets I can say that the gutenberg is much bigger and it performed better on average, hence, adding more data could help. (Also I belive a longer training could help but I did not managed to do it because I had a lot of sessions crashed.) But colab resourses would not allow it and also we should not use fiction text because it is more dificult to get semantics features out of those due to articstic forms and phrases. \n",
        "* **What happened with out-of-vocabulary words?**\n",
        "Mostly, they have failed in finding the pair. But due to big number of fails even with known words I would not say that this is a good conclusion.\n",
        "\n",
        "* **Which lexical semantic relations were picked up well, and which were not?**\n",
        "You can see the scores above, for different datasets it worked differently. Normalization of vector helps but I belive that it is the impact of testing pipeline assumpttions. However, it seems that on both datasets inflectional and lexicographic performs the best\n",
        "\n",
        "It also interesting that the answers for analogy tasks are extremely unstable with respect to the metric used, for instanse, LRCos works much faster (around an hour compared to 5 hours) but the outputs are much worse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVjTegIgabey"
      },
      "source": [
        "!mkdir Inflectional_morphology_results\n",
        "!mkdir Derivational_morphology_results\n",
        "!mkdir Encyclopedic_semantics_results\n",
        "!mkdir Lexicographic_semantics_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v5QQC2Mabxh",
        "outputId": "a09e10c0-9346-4008-ff77-a3cfacde8596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!python -m vecto benchmark analogy vocab/ BATS_3.0/1_Inflectional_morphology/ --path_out Inflectional_morphology_results/ --method 3CosAdd "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analogy ['vocab_guten/', 'BATS_3.0/1_Inflectional_morphology/', '--path_out', 'Inflectional_morphology_results_guten/', '--method', '3CosAdd']\n",
            "running  analogy\n",
            "SHAPE: (46909, 100)\n",
            "vocab size: 46909\n",
            "100% (50 of 50) |#########################| Elapsed Time: 1:01:58 Time:  1:01:58\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:55:34 Time:  0:55:34\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:49:33 Time:  0:49:33\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:41:56 Time:  0:41:56\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:34:36 Time:  0:34:36\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:28:59 Time:  0:28:59\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:23:39 Time:  0:23:39\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:17:23 Time:  0:17:23\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:11:51 Time:  0:11:51\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:05:57 Time:  0:05:57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV6XhJedOjKC",
        "outputId": "bb419c56-631c-492d-d25c-ae98e15931ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!python -m vecto benchmark analogy vocab/ BATS_3.0/2_Derivational_morphology/ --path_out Derivational_morphology_results/ --method 3CosAdd "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analogy ['vocab_guten/', 'BATS_3.0/2_Derivational_morphology/', '--path_out', 'Derivational_morphology_results_guten/', '--method', '3CosAdd']\n",
            "running  analogy\n",
            "SHAPE: (46909, 100)\n",
            "vocab size: 46909\n",
            "100% (50 of 50) |#########################| Elapsed Time: 1:08:24 Time:  1:08:24\n",
            "100% (50 of 50) |#########################| Elapsed Time: 1:01:25 Time:  1:01:25\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:55:26 Time:  0:55:26\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:49:43 Time:  0:49:43\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:42:27 Time:  0:42:27\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:35:02 Time:  0:35:02\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:27:15 Time:  0:27:15\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:19:50 Time:  0:19:50\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:13:19 Time:  0:13:19\n",
            "100% (50 of 50) |#########################| Elapsed Time: 0:06:34 Time:  0:06:34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V08ZSnj0Oj5y",
        "outputId": "a04cfee3-a583-4e3f-fbc0-11ca7a043ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "!python -m vecto benchmark analogy vocab/ BATS_3.0/3_Encyclopedic_semantics/ --path_out Encyclopedic_semantics_results/ --method 3CosAdd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analogy ['vocab_guten/', 'BATS_3.0/3_Encyclopedic_semantics/', '--path_out', 'Encyclopedic_semantics_results_guten/', '--method', '3CosAdd']\n",
            "running  analogy\n",
            "SHAPE: (46909, 100)\n",
            "vocab size: 46909\n",
            " 36% (18 of 50) |#########                | Elapsed Time: 0:02:46 ETA:   0:05:17"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foI9m9aAQaFl"
      },
      "source": [
        "!python -m vecto benchmark analogy vocab/ BATS_3.0/4_Lexicographic_semantics/ --path_out Lexicographic_semantics_results/ --method 3CosAdd"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}